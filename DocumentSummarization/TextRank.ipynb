{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import os\n",
    "import nltk\n",
    "import time\n",
    "import pickle\n",
    "import io\n",
    "import math\n",
    "import numpy as np\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from functools import reduce\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from scipy import spatial\n",
    "\n",
    "word_to_id_map=dict()\n",
    "id_to_word=dict()\n",
    "word_to_doc_freq=dict()\n",
    "sentence_to_id=dict()\n",
    "id_to_sentence=dict()\n",
    "sentence_count=0\n",
    "word_count=0\n",
    "total_docs=0\n",
    "sentence_tf_idf_vec=dict()\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "path=os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init():\n",
    "    global word_to_id_map\n",
    "    global id_to_word\n",
    "    global word_to_doc_freq\n",
    "    global sentence_to_id\n",
    "    global id_to_sentence\n",
    "    global sentence_count\n",
    "    global word_count\n",
    "    global total_docs\n",
    "    global sentence_tf_idf_vec\n",
    "    word_to_id_map=dict()\n",
    "    id_to_word=dict()\n",
    "    word_to_doc_freq=dict()\n",
    "    sentence_to_id=dict()\n",
    "    id_to_sentence=dict()\n",
    "    sentence_count=0\n",
    "    word_count=0\n",
    "    total_docs=0\n",
    "    sentence_tf_idf_vec=dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Module to parse the file and get the sentences\n",
    "\n",
    "def parse_file(topic_num):\n",
    "    global word_to_id_map\n",
    "    global id_to_word\n",
    "    global word_to_doc_freq\n",
    "    global sentence_to_id\n",
    "    global id_to_sentence\n",
    "    global sentence_count\n",
    "    global word_count\n",
    "    global total_docs\n",
    "    global sentence_tf_idf_vec\n",
    "    \n",
    "    for file in os.listdir(path+\"/Assignement2_IR/Topic\"+str(topic_num)):\n",
    "        total_docs+=1\n",
    "        f = io.open(os.path.join(path+\"/Assignement2_IR/Topic\"+str(topic_num), file), 'r', encoding='utf-8')\n",
    "        file_content=f.read()\n",
    "        soup = BeautifulSoup(file_content, \"lxml\")\n",
    "        text_group = soup.get_text()\n",
    "        text_group = ' '.join(text_group.strip().split('\\n'))\n",
    "        sentences=nltk.sent_tokenize(text_group)\n",
    "        doc_word_set=set()\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            tokens = nltk.word_tokenize(sentence)\n",
    "            tokens=[token.lower() for token in tokens]\n",
    "            wordset=[wordnet_lemmatizer.lemmatize(token) for token in tokens]\n",
    "            sentence_to_id[sentence]=sentence_count\n",
    "            id_to_sentence[sentence_count]=sentence\n",
    "            for word in wordset:\n",
    "                doc_word_set.add(word)\n",
    "                if word in word_to_id_map:\n",
    "                    ;\n",
    "                else:\n",
    "                    word_to_id_map[word]=word_count\n",
    "                    id_to_word[word_count]=word\n",
    "                    word_count+=1\n",
    "            sentence_count+=1\n",
    "        \n",
    "        for dist_word in doc_word_set:\n",
    "            if dist_word in word_to_doc_freq:\n",
    "                word_to_doc_freq[dist_word]+=1\n",
    "            else:\n",
    "                word_to_doc_freq[dist_word]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CREATING TF-IDF representation of the sentences\n",
    "\n",
    "def get_tf_idf_vec():\n",
    "    global word_to_id_map\n",
    "    global id_to_word\n",
    "    global word_to_doc_freq\n",
    "    global sentence_to_id\n",
    "    global id_to_sentence\n",
    "    global sentence_count\n",
    "    global word_count\n",
    "    global total_docs\n",
    "    global sentence_tf_idf_vec\n",
    "\n",
    "    for sentence in sentence_to_id:\n",
    "        tokens = nltk.word_tokenize(sentence)\n",
    "        tokens=[token.lower() for token in tokens]\n",
    "        wordset=[wordnet_lemmatizer.lemmatize(token) for token in tokens]\n",
    "        sentence_tf_idf_vec[sentence]=[0.0]*word_count\n",
    "        for word in wordset:\n",
    "            sentence_tf_idf_vec[sentence][word_to_id_map[word]]+=1\n",
    "        wordset=list(set(wordset))\n",
    "        for dist_word in wordset:\n",
    "            sentence_tf_idf_vec[sentence][word_to_id_map[dist_word]]*=math.log(total_docs/word_to_doc_freq[dist_word])/math.log(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Returns cosine similarity between the vector representation of the two sentences\n",
    "\n",
    "def get_cosine_similarity(sentence1,sentence2):\n",
    "    global word_to_id_map\n",
    "    global id_to_word\n",
    "    global word_to_doc_freq\n",
    "    global sentence_to_id\n",
    "    global id_to_sentence\n",
    "    global sentence_count\n",
    "    global word_count\n",
    "    global total_docs\n",
    "    global sentence_tf_idf_vec\n",
    "    sim=1 - spatial.distance.cosine(sentence_tf_idf_vec[sentence1], sentence_tf_idf_vec[sentence2])\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Constructs the similarity graph between sentences with threshold passed as param\n",
    "\n",
    "def construct_graph(threshold):\n",
    "    global word_to_id_map\n",
    "    global id_to_word\n",
    "    global word_to_doc_freq\n",
    "    global sentence_to_id\n",
    "    global id_to_sentence\n",
    "    global sentence_count\n",
    "    global word_count\n",
    "    global total_docs\n",
    "    global sentence_tf_idf_vec\n",
    "    graph=dict()\n",
    "    Degree=[0]*sentence_count\n",
    "    \n",
    "    for i in range(sentence_count):\n",
    "        graph[i]=[0.0]*sentence_count\n",
    "        for j in range (sentence_count):\n",
    "            graph[i][j]=get_cosine_similarity(id_to_sentence[i],id_to_sentence[j])\n",
    "            if(graph[i][j]<threshold):\n",
    "                graph[i][j]=0\n",
    "            else:\n",
    "                graph[i][j]=1\n",
    "                Degree[i]+=1\n",
    "    \n",
    "    for i in range(sentence_count):\n",
    "        for j in range (sentence_count):\n",
    "            if(Degree[i]!=0):\n",
    "                graph[i][j]/=Degree[i]\n",
    "                \n",
    "    return graph,Degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Power method to find the left eigen vector of the transition kernel\n",
    "\n",
    "def power_method(graph,damping_factor,tolerance):\n",
    "    d=damping_factor\n",
    "    global sentence_count\n",
    "    \n",
    "    tmp=list()\n",
    "    for key,value in graph.items():\n",
    "        tmp.append(value)\n",
    "    graph=np.asarray(tmp)\n",
    "    p_old=[1.0/sentence_count]*sentence_count\n",
    "    U=[p_old]*len(p_old)\n",
    "    U=np.asarray(U)\n",
    "    p_old=np.asarray(p_old).T\n",
    "    iter_count=0\n",
    "    while(True):\n",
    "        iter_count+=1\n",
    "        p=graph.T.dot(p_old)\n",
    "        p=(d*U+(1-d)*graph).T.dot(p_old)                     \n",
    "        if (  (np.sqrt(np.sum((p_old-p)**2))) <tolerance):\n",
    "            break\n",
    "        p_old=p\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Selects sentences on the basis of TextRank algorithm and writes them to a file\n",
    "\n",
    "def write_summary_to_file(topic_num,threshold):\n",
    "    global word_to_id_map\n",
    "    global id_to_word\n",
    "    global word_to_doc_freq\n",
    "    global sentence_to_id\n",
    "    global id_to_sentence\n",
    "    global sentence_count\n",
    "    global word_count\n",
    "    global total_docs\n",
    "    global sentence_tf_idf_vec\n",
    "    \n",
    "    init()\n",
    "    parse_file(topic_num)\n",
    "    get_tf_idf_vec()\n",
    "    graph,Degree=construct_graph(threshold)\n",
    "    \n",
    "    p=power_method(graph,0.15,1e-5)\n",
    "    words_added=0\n",
    "    summary=\"\"\n",
    "    p=np.argsort(p)\n",
    "    p=p[::-1]\n",
    "    for highest_degree_id in p:\n",
    "        sentence=id_to_sentence[highest_degree_id]\n",
    "        summary+=sentence\n",
    "        tokens = nltk.word_tokenize(sentence)\n",
    "        words_added+=len(tokens)\n",
    "        if(words_added>250):\n",
    "            break\n",
    "    \n",
    "    with open(\"TextRank_Summary_\"+str(topic_num)+\"_Threshold_\"+str(threshold)+\".txt\",'a') as file:\n",
    "        file.write(summary)\n",
    "    file.close()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_list=[0.1,0.2,0.3]\n",
    "for topic in range(1,6):\n",
    "    for thresh in threshold_list:\n",
    "        write_summary_to_file(topic,thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
